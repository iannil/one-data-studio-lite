"""
Create example ETL pipelines for production data sources.

Creates 2 example pipelines for each data system:
- Finance: äº¤æ˜“æ±‡æ€»æŠ¥è¡¨, é£é™©æŒ‡æ ‡è®¡ç®—
- IoT: è®¾å¤‡çŠ¶æ€èšåˆ, å‘Šè­¦ç»Ÿè®¡åˆ†æ
- HR: è–ªèµ„æœˆåº¦æŠ¥è¡¨, è€ƒå‹¤ç»Ÿè®¡åˆ†æ
- Medical: å°±è¯Šç»Ÿè®¡åˆ†æ, å¤„æ–¹åˆ†ææŠ¥è¡¨

Usage:
    cd backend
    source .venv/bin/activate
    python scripts/create_example_pipelines.py
"""

from __future__ import annotations

import asyncio
import sys
from pathlib import Path
from uuid import UUID

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker

from app.core.config import settings
from app.models.metadata import DataSource
from app.models.etl import ETLPipeline, ETLStep, ETLStepType, PipelineStatus


# Pipeline definitions - will be populated with actual source IDs
def get_pipeline_definitions(sources: dict[str, UUID]) -> list[dict]:
    """Get pipeline definitions with actual source IDs."""
    return [
        # ===============================
        # Finance Pipelines
        # ===============================
        {
            "name": "é‡‘èäº¤æ˜“æ—¥æ±‡æ€»æŠ¥è¡¨",
            "description": "æŒ‰æ—¥æœŸæ±‡æ€»äº¤æ˜“æ•°æ®ï¼Œè®¡ç®—æ¯æ—¥äº¤æ˜“é‡ã€äº¤æ˜“é‡‘é¢ã€æ‰‹ç»­è´¹ç­‰æŒ‡æ ‡",
            "source_type": "query",
            "source_config": {
                "source_id": str(sources["finance"]),
                "query": """
                    SELECT
                        DATE(transaction_date) as trade_date,
                        transaction_type,
                        COUNT(*) as transaction_count,
                        SUM(amount) as total_amount,
                        AVG(amount) as avg_amount,
                        SUM(fee) as total_fee,
                        COUNT(DISTINCT account_id) as unique_accounts
                    FROM finance.transactions
                    WHERE status = 'completed'
                    GROUP BY DATE(transaction_date), transaction_type
                    ORDER BY trade_date DESC
                """,
            },
            "target_type": "table",
            "target_config": {
                "source_id": str(sources["finance"]),
                "table_name": "finance.daily_transaction_summary",
                "if_exists": "replace",
            },
            "tags": ["finance", "report", "daily"],
            "steps": [
                {
                    "name": "æŒ‰æ—¥æœŸæ’åº",
                    "step_type": ETLStepType.SORT,
                    "config": {"columns": ["trade_date"], "ascending": False},
                    "order": 1,
                },
                {
                    "name": "è®¡ç®—äº¤æ˜“å æ¯”",
                    "step_type": ETLStepType.CALCULATE,
                    "config": {
                        "calculations": [
                            {
                                "column": "fee_ratio",
                                "expression": "total_fee / total_amount * 100",
                            }
                        ]
                    },
                    "order": 2,
                },
            ],
        },
        {
            "name": "å®¢æˆ·é£é™©è¯„åˆ†åˆ†æ",
            "description": "åˆ†æå®¢æˆ·é£é™©è¯„ä¼°æ•°æ®ï¼ŒæŒ‰é£é™©ç­‰çº§åˆ†ç»„ç»Ÿè®¡",
            "source_type": "query",
            "source_config": {
                "source_id": str(sources["finance"]),
                "query": """
                    SELECT
                        c.customer_type,
                        r.risk_level,
                        r.risk_category,
                        COUNT(*) as customer_count,
                        AVG(r.risk_score) as avg_risk_score,
                        MIN(r.assessment_date) as earliest_assessment,
                        MAX(r.assessment_date) as latest_assessment
                    FROM finance.risk_assessments r
                    JOIN finance.customers c ON r.customer_id = c.id
                    WHERE r.is_active = true
                    GROUP BY c.customer_type, r.risk_level, r.risk_category
                """,
            },
            "target_type": "table",
            "target_config": {
                "source_id": str(sources["finance"]),
                "table_name": "finance.risk_analysis_report",
                "if_exists": "replace",
            },
            "tags": ["finance", "risk", "analysis"],
            "steps": [
                {
                    "name": "é£é™©ç­‰çº§æ˜ å°„",
                    "step_type": ETLStepType.MAP_VALUES,
                    "config": {
                        "column": "risk_level",
                        "mapping": {
                            "low": "ä½é£é™©",
                            "medium": "ä¸­é£é™©",
                            "high": "é«˜é£é™©",
                            "critical": "æé«˜é£é™©",
                        },
                    },
                    "order": 1,
                },
                {
                    "name": "æŒ‰é£é™©è¯„åˆ†æ’åº",
                    "step_type": ETLStepType.SORT,
                    "config": {"columns": ["avg_risk_score"], "ascending": False},
                    "order": 2,
                },
            ],
        },
        # ===============================
        # IoT Pipelines
        # ===============================
        {
            "name": "è®¾å¤‡çŠ¶æ€å®æ—¶èšåˆ",
            "description": "èšåˆè®¾å¤‡æœ€æ–°çŠ¶æ€æ•°æ®ï¼ŒæŒ‰è®¾å¤‡ç±»å‹å’ŒçŠ¶æ€åˆ†ç»„ç»Ÿè®¡",
            "source_type": "query",
            "source_config": {
                "source_id": str(sources["iot"]),
                "query": """
                    SELECT
                        dt.type_name as device_type,
                        d.status,
                        d.location,
                        COUNT(*) as device_count,
                        COUNT(CASE WHEN d.is_online = true THEN 1 END) as online_count,
                        AVG(EXTRACT(EPOCH FROM (NOW() - d.last_heartbeat))/3600) as avg_hours_since_heartbeat
                    FROM iot.devices d
                    JOIN iot.device_types dt ON d.device_type_id = dt.id
                    GROUP BY dt.type_name, d.status, d.location
                """,
            },
            "target_type": "table",
            "target_config": {
                "source_id": str(sources["iot"]),
                "table_name": "iot.device_status_summary",
                "if_exists": "replace",
            },
            "tags": ["iot", "device", "status"],
            "steps": [
                {
                    "name": "è®¡ç®—åœ¨çº¿ç‡",
                    "step_type": ETLStepType.CALCULATE,
                    "config": {
                        "calculations": [
                            {
                                "column": "online_rate",
                                "expression": "online_count / device_count * 100",
                            }
                        ]
                    },
                    "order": 1,
                },
                {
                    "name": "æŒ‰è®¾å¤‡æ•°é‡æ’åº",
                    "step_type": ETLStepType.SORT,
                    "config": {"columns": ["device_count"], "ascending": False},
                    "order": 2,
                },
            ],
        },
        {
            "name": "å‘Šè­¦ç»Ÿè®¡åˆ†ææŠ¥è¡¨",
            "description": "æŒ‰å‘Šè­¦çº§åˆ«å’Œç±»å‹ç»Ÿè®¡å‘Šè­¦æ•°æ®ï¼Œåˆ†æå‘Šè­¦è¶‹åŠ¿",
            "source_type": "query",
            "source_config": {
                "source_id": str(sources["iot"]),
                "query": """
                    SELECT
                        DATE(a.triggered_at) as alert_date,
                        a.severity,
                        a.alert_type,
                        COUNT(*) as alert_count,
                        COUNT(CASE WHEN a.is_resolved = true THEN 1 END) as resolved_count,
                        AVG(EXTRACT(EPOCH FROM (a.resolved_at - a.triggered_at))/60) as avg_resolution_minutes
                    FROM iot.alerts a
                    WHERE a.triggered_at >= NOW() - INTERVAL '30 days'
                    GROUP BY DATE(a.triggered_at), a.severity, a.alert_type
                    ORDER BY alert_date DESC
                """,
            },
            "target_type": "table",
            "target_config": {
                "source_id": str(sources["iot"]),
                "table_name": "iot.alert_statistics",
                "if_exists": "replace",
            },
            "tags": ["iot", "alert", "statistics"],
            "steps": [
                {
                    "name": "å‘Šè­¦çº§åˆ«æ˜ å°„",
                    "step_type": ETLStepType.MAP_VALUES,
                    "config": {
                        "column": "severity",
                        "mapping": {
                            "info": "ä¿¡æ¯",
                            "warning": "è­¦å‘Š",
                            "error": "é”™è¯¯",
                            "critical": "ä¸¥é‡",
                        },
                    },
                    "order": 1,
                },
                {
                    "name": "è®¡ç®—è§£å†³ç‡",
                    "step_type": ETLStepType.CALCULATE,
                    "config": {
                        "calculations": [
                            {
                                "column": "resolution_rate",
                                "expression": "resolved_count / alert_count * 100",
                            }
                        ]
                    },
                    "order": 2,
                },
            ],
        },
        # ===============================
        # HR Pipelines
        # ===============================
        {
            "name": "å‘˜å·¥è–ªèµ„æœˆåº¦ç»Ÿè®¡",
            "description": "æŒ‰éƒ¨é—¨æ±‡æ€»æœˆåº¦è–ªèµ„æ•°æ®ï¼Œè®¡ç®—å¹³å‡è–ªèµ„ã€æ€»æˆæœ¬ç­‰æŒ‡æ ‡",
            "source_type": "query",
            "source_config": {
                "source_id": str(sources["hr"]),
                "query": """
                    SELECT
                        d.name as department_name,
                        DATE_FORMAT(s.pay_date, '%Y-%m') as pay_month,
                        COUNT(DISTINCT s.employee_id) as employee_count,
                        SUM(s.gross_salary) as total_gross,
                        AVG(s.gross_salary) as avg_gross,
                        SUM(s.net_salary) as total_net,
                        SUM(s.total_deductions) as total_deductions,
                        SUM(s.bonus) as total_bonus,
                        SUM(s.overtime_pay) as total_overtime
                    FROM hr_system.salary_records s
                    JOIN hr_system.employees e ON s.employee_id = e.id
                    JOIN hr_system.departments d ON e.department_id = d.id
                    WHERE s.payment_status = 'paid'
                    GROUP BY d.name, DATE_FORMAT(s.pay_date, '%Y-%m')
                    ORDER BY pay_month DESC, total_gross DESC
                """,
            },
            "target_type": "table",
            "target_config": {
                "source_id": str(sources["hr"]),
                "table_name": "monthly_salary_report",
                "if_exists": "replace",
            },
            "tags": ["hr", "salary", "monthly"],
            "steps": [
                {
                    "name": "è®¡ç®—äººå‡å¥–é‡‘",
                    "step_type": ETLStepType.CALCULATE,
                    "config": {
                        "calculations": [
                            {
                                "column": "avg_bonus",
                                "expression": "total_bonus / employee_count",
                            }
                        ]
                    },
                    "order": 1,
                },
                {
                    "name": "æŒ‰æ€»è–ªèµ„æ’åº",
                    "step_type": ETLStepType.SORT,
                    "config": {"columns": ["pay_month", "total_gross"], "ascending": [False, False]},
                    "order": 2,
                },
            ],
        },
        {
            "name": "å‘˜å·¥è€ƒå‹¤åˆ†ææŠ¥è¡¨",
            "description": "æŒ‰éƒ¨é—¨ç»Ÿè®¡è€ƒå‹¤æ•°æ®ï¼Œåˆ†æè¿Ÿåˆ°ã€æ—©é€€ã€ç¼ºå‹¤æƒ…å†µ",
            "source_type": "query",
            "source_config": {
                "source_id": str(sources["hr"]),
                "query": """
                    SELECT
                        d.name as department_name,
                        DATE_FORMAT(a.attendance_date, '%Y-%m') as month,
                        COUNT(*) as total_records,
                        SUM(CASE WHEN a.is_late = 1 THEN 1 ELSE 0 END) as late_count,
                        SUM(CASE WHEN a.is_early_leave = 1 THEN 1 ELSE 0 END) as early_leave_count,
                        SUM(CASE WHEN a.is_absent = 1 THEN 1 ELSE 0 END) as absent_count,
                        AVG(a.work_hours) as avg_work_hours,
                        SUM(a.overtime_hours) as total_overtime_hours
                    FROM hr_system.attendance a
                    JOIN hr_system.employees e ON a.employee_id = e.id
                    JOIN hr_system.departments d ON e.department_id = d.id
                    GROUP BY d.name, DATE_FORMAT(a.attendance_date, '%Y-%m')
                    ORDER BY month DESC
                """,
            },
            "target_type": "table",
            "target_config": {
                "source_id": str(sources["hr"]),
                "table_name": "attendance_analysis_report",
                "if_exists": "replace",
            },
            "tags": ["hr", "attendance", "analysis"],
            "steps": [
                {
                    "name": "è®¡ç®—å‡ºå‹¤ç‡",
                    "step_type": ETLStepType.CALCULATE,
                    "config": {
                        "calculations": [
                            {
                                "column": "attendance_rate",
                                "expression": "(total_records - absent_count) / total_records * 100",
                            },
                            {
                                "column": "late_rate",
                                "expression": "late_count / total_records * 100",
                            },
                        ]
                    },
                    "order": 1,
                },
            ],
        },
        # ===============================
        # Medical Pipelines
        # ===============================
        {
            "name": "é—¨è¯Šå°±è¯Šç»Ÿè®¡åˆ†æ",
            "description": "æŒ‰ç§‘å®¤å’ŒåŒ»ç”Ÿç»Ÿè®¡é—¨è¯Šå°±è¯Šæ•°æ®ï¼Œåˆ†æå°±è¯Šé‡å’Œæ‚£è€…æ»¡æ„åº¦",
            "source_type": "query",
            "source_config": {
                "source_id": str(sources["medical"]),
                "query": """
                    SELECT
                        h.name as hospital_name,
                        dep.name as department_name,
                        doc.name as doctor_name,
                        DATE_FORMAT(a.appointment_date, '%Y-%m') as month,
                        COUNT(*) as appointment_count,
                        SUM(CASE WHEN a.status = 'completed' THEN 1 ELSE 0 END) as completed_count,
                        SUM(CASE WHEN a.status = 'cancelled' THEN 1 ELSE 0 END) as cancelled_count,
                        SUM(CASE WHEN a.status = 'no_show' THEN 1 ELSE 0 END) as no_show_count
                    FROM medical.appointments a
                    JOIN medical.doctors doc ON a.doctor_id = doc.id
                    JOIN medical.departments dep ON doc.department_id = dep.id
                    JOIN medical.hospitals h ON dep.hospital_id = h.id
                    GROUP BY h.name, dep.name, doc.name, DATE_FORMAT(a.appointment_date, '%Y-%m')
                    ORDER BY month DESC, appointment_count DESC
                """,
            },
            "target_type": "table",
            "target_config": {
                "source_id": str(sources["medical"]),
                "table_name": "outpatient_statistics",
                "if_exists": "replace",
            },
            "tags": ["medical", "outpatient", "statistics"],
            "steps": [
                {
                    "name": "è®¡ç®—å®Œæˆç‡",
                    "step_type": ETLStepType.CALCULATE,
                    "config": {
                        "calculations": [
                            {
                                "column": "completion_rate",
                                "expression": "completed_count / appointment_count * 100",
                            },
                            {
                                "column": "no_show_rate",
                                "expression": "no_show_count / appointment_count * 100",
                            },
                        ]
                    },
                    "order": 1,
                },
            ],
        },
        {
            "name": "å¤„æ–¹ç”¨è¯åˆ†ææŠ¥è¡¨",
            "description": "åˆ†æå¤„æ–¹è¯å“ä½¿ç”¨æƒ…å†µï¼ŒæŒ‰è¯å“ç±»åˆ«ç»Ÿè®¡ç”¨é‡å’Œé‡‘é¢",
            "source_type": "query",
            "source_config": {
                "source_id": str(sources["medical"]),
                "query": """
                    SELECT
                        pi.drug_category,
                        pi.drug_name,
                        DATE_FORMAT(p.prescription_date, '%Y-%m') as month,
                        COUNT(DISTINCT p.id) as prescription_count,
                        SUM(pi.quantity) as total_quantity,
                        SUM(pi.unit_price * pi.quantity) as total_amount,
                        AVG(pi.unit_price) as avg_unit_price,
                        COUNT(DISTINCT p.patient_id) as unique_patients
                    FROM medical.prescription_items pi
                    JOIN medical.prescriptions p ON pi.prescription_id = p.id
                    WHERE p.status = 'dispensed'
                    GROUP BY pi.drug_category, pi.drug_name, DATE_FORMAT(p.prescription_date, '%Y-%m')
                    ORDER BY month DESC, total_amount DESC
                """,
            },
            "target_type": "table",
            "target_config": {
                "source_id": str(sources["medical"]),
                "table_name": "prescription_analysis_report",
                "if_exists": "replace",
            },
            "tags": ["medical", "prescription", "analysis"],
            "steps": [
                {
                    "name": "è®¡ç®—å¹³å‡å¤„æ–¹é‡‘é¢",
                    "step_type": ETLStepType.CALCULATE,
                    "config": {
                        "calculations": [
                            {
                                "column": "avg_prescription_amount",
                                "expression": "total_amount / prescription_count",
                            }
                        ]
                    },
                    "order": 1,
                },
                {
                    "name": "æŒ‰é‡‘é¢æ’åº",
                    "step_type": ETLStepType.SORT,
                    "config": {"columns": ["total_amount"], "ascending": False},
                    "order": 2,
                },
            ],
        },
    ]


async def create_pipelines() -> None:
    """Create example ETL pipelines."""
    engine = create_async_engine(settings.DATABASE_URL, echo=False)
    async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

    print("\n" + "=" * 60)
    print("åˆ›å»ºç¤ºä¾‹ ETL ç®¡é“")
    print("=" * 60)

    async with async_session() as session:
        # Get data source IDs
        result = await session.execute(select(DataSource))
        all_sources = list(result.scalars())

        # Map source names to IDs
        source_map = {}
        for source in all_sources:
            if "Finance" in source.name or "é‡‘è" in source.name:
                source_map["finance"] = source.id
            elif "IoT" in source.name or "ç‰©è”ç½‘" in source.name:
                source_map["iot"] = source.id
            elif "HR" in source.name or "äººåŠ›èµ„æº" in source.name:
                source_map["hr"] = source.id
            elif "Medical" in source.name or "åŒ»ç–—" in source.name:
                source_map["medical"] = source.id

        if len(source_map) < 4:
            print(f"\nâŒ é”™è¯¯: æœªæ‰¾åˆ°æ‰€æœ‰æ•°æ®æºã€‚å½“å‰æ‰¾åˆ°: {list(source_map.keys())}")
            print("è¯·å…ˆè¿è¡Œ register_production_sources.py")
            return

        print(f"\næ‰¾åˆ°æ•°æ®æº:")
        for name, source_id in source_map.items():
            print(f"  - {name}: {source_id}")

        # Get pipeline definitions
        pipelines_def = get_pipeline_definitions(source_map)

        created_count = 0
        skipped_count = 0

        for pipe_def in pipelines_def:
            # Check if pipeline already exists
            result = await session.execute(
                select(ETLPipeline).where(ETLPipeline.name == pipe_def["name"])
            )
            existing = result.scalar_one_or_none()

            if existing:
                print(f"\nâ­ï¸  è·³è¿‡: {pipe_def['name']} (å·²å­˜åœ¨)")
                skipped_count += 1
                continue

            # Create pipeline
            pipeline = ETLPipeline(
                name=pipe_def["name"],
                description=pipe_def["description"],
                source_type=pipe_def["source_type"],
                source_config=pipe_def["source_config"],
                target_type=pipe_def["target_type"],
                target_config=pipe_def["target_config"],
                tags=pipe_def["tags"],
                status=PipelineStatus.ACTIVE,
            )
            session.add(pipeline)
            await session.flush()

            # Create steps
            for step_def in pipe_def.get("steps", []):
                step = ETLStep(
                    pipeline_id=pipeline.id,
                    name=step_def["name"],
                    step_type=step_def["step_type"],
                    config=step_def["config"],
                    order=step_def["order"],
                    is_enabled=True,
                )
                session.add(step)

            print(f"\nâœ… å·²åˆ›å»º: {pipe_def['name']}")
            print(f"   æ ‡ç­¾: {', '.join(pipe_def['tags'])}")
            print(f"   æ­¥éª¤æ•°: {len(pipe_def.get('steps', []))}")

            created_count += 1

        await session.commit()

        print("\n" + "-" * 60)
        print(f"å®Œæˆ: åˆ›å»º {created_count} ä¸ªç®¡é“, è·³è¿‡ {skipped_count} ä¸ª")
        print("=" * 60)

        # List all pipelines
        result = await session.execute(select(ETLPipeline))
        all_pipelines = list(result.scalars())

        print("\nğŸ“‹ å½“å‰æ‰€æœ‰ ETL ç®¡é“:")
        print("-" * 60)
        for pipeline in all_pipelines:
            status_icon = "ğŸŸ¢" if pipeline.status == PipelineStatus.ACTIVE else "ğŸ”´"
            print(f"  {status_icon} {pipeline.name}")
            print(f"      æ ‡ç­¾: {', '.join(pipeline.tags)}")
        print()

    await engine.dispose()


if __name__ == "__main__":
    asyncio.run(create_pipelines())
